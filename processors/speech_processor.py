import numpy as np
from rclpy.node import Node
from collections import deque
import time
import os
import soundfile as sf
from datetime import datetime


class SpeechProcessor:
    """
    ìŒì„± ì²˜ë¦¬ê¸° - ì§ì ‘ ë²„í¼ë§ ë°©ì‹
    
    VAD ê²°ê³¼ë¥¼ ë°›ì•„ì„œ ë°œí™” ì‹œì‘/ëì„ íŒë‹¨í•˜ê³ ,
    ì˜¤ë””ì˜¤ ë°ì´í„°ë¥¼ ì§ì ‘ ë²„í¼ì— ì €ì¥í•˜ì—¬ ì™„ì„±ëœ ë°œí™” êµ¬ê°„ì„ WAVë¡œ ì €ì¥í•©ë‹ˆë‹¤.
    """
    
    def __init__(self, node: Node, vad_processor, energy_monitor, audio_manager, use_stream_mode=False):
        """ìŒì„± ì²˜ë¦¬ê¸° ì´ˆê¸°í™”"""
        self.node = node
        self.vad_processor = vad_processor
        self.energy_monitor = energy_monitor
        self.audio_manager = audio_manager
        self.use_stream_mode = use_stream_mode

        # configì—ì„œ ì„¤ì •ê°’ ë¡œë“œ
        config = node.config
        config_vad = config.get('vad', {})
        
        # VAD ê´€ë ¨ ì„¤ì •
        self.sample_rate = config_vad.get('sample_rate', 48000)
        self.frame_duration = config_vad.get('frame_duration', 10) / 1000.0  # ms to seconds
        self.vad_buffer_size = config_vad.get('buffer_size', 5)        
        self.vad_threshold = config_vad.get('threshold', 0.5)
        self.vad_high_threshold = config_vad.get('high_threshold', 0.98)
        
        # ì—ë„ˆì§€ ë ˆë²¨ ë° ìŒì„±ì²˜ë¦¬ ê´€ë ¨ ì„¤ì •
        self.noise_db_threshold = config.get('noise_db_threshold', -35)
        self.min_db_threshold = config.get('min_db_threshold', -45)

        # ì‹œê°„ ì„¤ì •
        min_speech_duration = config.get('min_speech_duration', 0.1)     # ìµœì†Œ ë°œí™” ê¸¸ì´ (ì´ˆ)
        max_speech_duration = config.get('max_speech_duration', 30.0)   # ìµœëŒ€ ë°œí™” ê¸¸ì´ (ì´ˆ)
        pause_duration = config.get('pause_duration', 0.75)             # ë°œí™” ì¢…ë£Œ ì¹¨ë¬µ ì‹œê°„ (ì´ˆ)
        pre_speech_padding = config.get('padding_duration', 0.3)        # ë°œí™” ì „ íŒ¨ë”© (ì´ˆ)
        
        # íŒŒì¼ ì €ì¥ ì„¤ì •
        self.speech_segments_dir = config.get('speech_segments_dir', '.aeirobot_asr/speech_logs')

        # ìƒ˜í”Œ ìˆ˜ë¡œ ë³€í™˜
        self.min_speech_samples = int(self.sample_rate * min_speech_duration)
        self.max_speech_samples = int(self.sample_rate * max_speech_duration)
        self.pause_samples = int(self.sample_rate * pause_duration)
        self.pre_speech_samples = int(self.sample_rate * pre_speech_padding)

        # ë²„í¼ë§ì„ ìœ„í•œ ë²„í¼ë“¤
        self.frame_buffer = deque(maxlen=500)                        # ëª¨ë“  í”„ë ˆì„ ì„ì‹œ ì €ì¥ (ìµœê·¼ 500ê°œ)
        self.vad_buffer = deque(maxlen=self.vad_buffer_size)
        # self.speech_buffer = []                                          # í˜„ì¬ ë°œí™” ì˜¤ë””ì˜¤ ë²„í¼ (float32 ìƒ˜í”Œ)
        self.pre_speech_buffer = deque(maxlen=self.pre_speech_samples)   # ë°œí™” ì „ íŒ¨ë”©ìš© ë§ë²„í¼
        self.pending_speech_buffer = []                                  # ë°œí™” ê°ì§€ ëŒ€ê¸° ì¤‘ ì„ì‹œ ë²„í¼
        
        # ì‹œê°„ ì¸¡ì •ìš©
        self.speech_start_time = None         # ë°œí™” ì‹œì‘ ì‹œê°„
        self.last_voice_time = None           # ë§ˆì§€ë§‰ ìŒì„± ì‹œê°„ (ì˜›ë‚  ì½”ë“œ)

        # ìƒíƒœ ë³€ìˆ˜
        self.is_speaking = False
        self.speech_sample_count = 0          # ì—°ì† ë°œí™” ìƒ˜í”Œ ìˆ˜
        self.silence_sample_count = 0         # ì—°ì† ì¹¨ë¬µ ìƒ˜í”Œ ìˆ˜
       
        # ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
        self.last_log_time = time.time()
        self.log_interval = 5.0

        self.node.get_logger().info(
            f'Speech processor initialized (Direct Buffering Mode):\n'
            f'Sample rate: {self.sample_rate} Hz\n'
            f'Frame duration: {self.frame_duration * 1000:.1f} ms\n'
            f'Min speech duration: {min_speech_duration:.3f}s ({self.min_speech_samples} samples)\n'
            f'Max speech duration: {max_speech_duration:.1f}s ({self.max_speech_samples} samples)\n'
            f'Pause duration: {pause_duration:.3f}s ({self.pause_samples} samples)\n'
            f'Pre-speech padding: {pre_speech_padding:.3f}s ({self.pre_speech_samples} samples)\n'
            f'VAD threshold: {self.vad_threshold}\n'
            f'Noise threshold: {self.noise_db_threshold} dB'
        )

    # ===============================================================================================================================================

    def _reset_state(self):
        """ìƒíƒœ ì´ˆê¸°í™”"""
        self.is_speaking = False
        self.speech_start_time = None
        self.last_voice_time = None
        self.frame_buffer.clear()
        self.vad_buffer.clear()
        # self.speech_buffer = []
        self.speech_sample_count = 0
        self.silence_sample_count = 0


    def _check_voice_activity(self, smoothed_vad_score, db_level):
        """ìŒì„± í™œì„± ìƒíƒœë¥¼ ì²´í¬í•˜ëŠ” í•¨ìˆ˜"""
        # ë†’ì€ ì‹ ë¢°ë„ ì¡°ê±´ ì²´í¬ - VAD ì ìˆ˜ê°€ ë§¤ìš° ë†’ê³  ìµœì†Œ DB levelì€ ë„˜ì„ ë•Œ
        if smoothed_vad_score >= self.vad_high_threshold and db_level >= self.min_db_threshold:
            return True, 'high'

        # DB levelì´ ê¸°ì¤€ ì´í•˜ë©´ ìŒì„±ì´ ì•„ë‹˜
        if db_level < self.noise_db_threshold:
            # í˜„ì¬ ë§í•˜ëŠ” ì¤‘ì´ë©´ ë§¤ìš° ê´€ëŒ€í•œ ê¸°ì¤€ ì ìš©
            if self.is_speaking and smoothed_vad_score >= self.vad_threshold * 0.7:
                return True, 'low'
            return False, 'none'

        # ê¸°ë³¸ ì‹ ë¢°ë„ ì¡°ê±´ ì²´í¬ - DB levelì´ ì¶©ë¶„í•˜ê³  VADë„ ê¸°ì¤€ ì´ìƒ
        if smoothed_vad_score >= self.vad_threshold:
            return True, 'normal'

        return False, 'none'

    def process_frame(self, raw_data, encoding='F32LE'):
        """ROS2 ì˜¤ë””ì˜¤ ë©”ì‹œì§€ ì²˜ë¦¬ - ì˜›ë‚  ì½”ë“œ ë°©ì‹ (ëª¨ë“  í”„ë ˆì„ ë²„í¼ë§)

        Args:
            raw_data: ROS2 Audio ë©”ì‹œì§€ì˜ raw data (bytes, list, array ë“±)
            encoding: ì˜¤ë””ì˜¤ ì¸ì½”ë”© í˜•ì‹ (ì˜ˆ: 'F32LE', 'S16LE')

        Returns:
            tuple: (result, db_level, vad_score)
        """
        try:
            # 1) VAD ë° dB ê³„ì‚°ìš© float32 í”„ë ˆì„
            ## preprocessí•´ì„œ float32ë¡œ ë³€í™˜ 
            frame = self.audio_manager.preprocess_audio(raw_data, encoding)
            num_samples = len(frame)
            ## AudioDeviceManagerì˜ ë²„í¼ì— ì¶”ê°€
            self.audio_manager.add_to_buffer(frame)
            db_level = self.energy_monitor.calculate_energy(frame)
            
            # 2) VAD ì²˜ë¦¬
            vad_result = self.vad_processor.process_frame(frame)
            if vad_result is None:
                return None, db_level, 0.0

            is_speech, smoothed_prob, vad_score = vad_result
            
            # VAD ê²°ê³¼ ë²„í¼ì— ì¶”ê°€ ë° ìŠ¤ë¬´ë”©
            self.vad_buffer.append(vad_score)
            smoothed_vad_score = sum(self.vad_buffer) / len(self.vad_buffer)

            # ìŒì„± í™œì„± ìƒíƒœ ì²´í¬, confidenceëŠ” 'none', 'low', 'normal', 'high'
            is_voice, confidence = self._check_voice_activity(smoothed_vad_score, db_level)
            
            if confidence in ['low']:   # dbì´ ë„ˆë¬´ ë‚®ì•„ì„œ confidenceê°€ ì„ê³„ê°’ ì´í•˜ë¡œ ë‚®ìœ¼ë©´ ë¬´ì‹œ
                return None, db_level, vad_score
            
            result = self._speech_segmentor(raw_data, is_voice, num_samples)
            return result, db_level, vad_score

        except Exception as e:
            self.node.get_logger().error(f'Error processing frame: {str(e)}')
            import traceback
            traceback.print_exc()
            return None, -80.0, 0.0

    
    def _speech_segmentor(self, raw_data, is_voice, num_samples):
        # raw_data = bytes(raw_data)
        result = None
        if not self.is_speaking:
            # ===== ë°œí™” ì¤‘ì´ ì•„ë‹ ë•Œ =====
            # ëª¨ë“  í”„ë ˆì„ì„ frame_bufferì— ì„ì‹œ ì €ì¥
            self.frame_buffer.append(raw_data)
            
            if is_voice:
                self.speech_sample_count += num_samples
                self.silence_sample_count = 0
                
                # ìµœì†Œ ë°œí™” ê¸¸ì´ ë„ë‹¬ ì‹œ ë°œí™” ì‹œì‘
                if self.speech_sample_count >= self.min_speech_samples:
                    self.is_speaking = True
                    
                    # ë²„í¼ ê¸¸ì´ë§Œí¼ ë¹¼ì„œ ë°œí™” ì‹œì‘ ì‹œê°„ ê³„ì‚° (íŒ¨ë”© í¬í•¨)
                    current_time = time.time()
                    self.speech_start_time = current_time - (len(self.frame_buffer) * self.frame_duration * 0.2)
                    self.last_voice_time = current_time  # ì´ˆê¸°í™” (ë°œí™” ì‹œì‘ ì‹œì )
                    
                    # # frame_buffer ì „ì²´ë¥¼ speech_bufferë¡œ ì´ë™
                    # self.speech_buffer = list(self.frame_buffer)
                    self.frame_buffer.clear()
                    
                    # ë””ìŠ¤í”Œë ˆì´ ì´ë²¤íŠ¸ ì—…ë°ì´íŠ¸ 
                    if hasattr(self.node, 'display_lock'):
                        with self.node.display_lock:
                            self.node.last_event = f"ğŸ¤ ìŒì„± ì‹œì‘)"
                    
                    self.node.get_logger().debug(f'Speech started')
            else:
                self.speech_sample_count = 0

        else:
            # ===== ë°œí™” ì¤‘ì¼ ë•Œ - ì‹œê°„ ê¸°ë¡  =====
            current_time = time.time()
            
            if is_voice:
                self.last_voice_time = current_time  # ë§ˆì§€ë§‰ ìŒì„± ì‹œê°„ ì—…ë°ì´íŠ¸
                self.silence_sample_count = 0
                
            else:
                self.silence_sample_count += num_samples

                # ì¹¨ë¬µì´ ì¼ì • ì‹œê°„ ì§€ì†ë˜ë©´ ë°œí™” ì¢…ë£Œ
                if self.silence_sample_count >= self.pause_samples:
                    # ë§ˆì§€ë§‰ ìŒì„± ì‹œê°„ì„ ì—¬ìœ ìˆê²Œ ì„¤ì • -> ì¹¨ë¬µ: 0.6ì´ˆë¼ë©´ ê·¸ ì ˆë°˜ì¸ 0.3ì´ˆê¹Œì§€ ì—¬ìœ ìˆê²Œ í¬í•¨
                    self.last_voice_time = current_time - (self.silence_sample_count / self.sample_rate * 0.5)

                    if hasattr(self.node, 'display_lock'):
                        with self.node.display_lock:
                            self.node.last_event = "ğŸ”‡ ìŒì„± ì¢…ë£Œ"
                    
                    result = self._finalize_speech()
                    return result
            
            # ìµœëŒ€ ë°œí™” ê¸¸ì´ ì´ˆê³¼ ì‹œ ê°•ì œ ì¢…ë£Œ
            if self.speech_start_time is not None:
                elapsed = current_time - self.speech_start_time
                if elapsed >= (self.max_speech_samples / self.sample_rate):
                    self.last_voice_time = current_time
                    self.node.get_logger().info('Maximum speech duration reached, forcing finalization')
                    result = self._finalize_speech()
                    return result
        
    
    
    def _finalize_speech(self):
        """ROS2 ë°œí™” ì¢…ë£Œ ì²˜ë¦¬ - ì‹œê°„ ê¸°ë°˜ ì¶”ì¶œ (ì˜›ë‚  ì½”ë“œ ë°©ì‹)"""
        try:
            # ë°œí™” ì§€ì† ì‹œê°„ ì²´í¬
            speech_duration = self.last_voice_time - self.speech_start_time
            min_duration = self.min_speech_samples / self.sample_rate
            
            if speech_duration < min_duration:
                self.node.get_logger().info(
                    f'Speech too short ({speech_duration:.3f}s < {min_duration:.3f}s), discarding'
                )
                self._reset_state()
                return None

            if self.use_stream_mode:
                # ìŠ¤íŠ¸ë¦¼ ëª¨ë“œ: íŒŒì¼ ì €ì¥ ì—†ì´ ì˜¤ë””ì˜¤ ë°ì´í„°ë§Œ ì¶”ì¶œ
                result = self.audio_manager.extract_speech_segment_data(
                    self.speech_start_time,
                    self.last_voice_time
                )
                
                if result:
                    speech_segment, sample_rate = result
                    self.node.get_logger().info(f'Speech segment extracted for streaming: {len(speech_segment)} samples')
                    self._reset_state()
                    return True, (speech_segment, sample_rate)
            else:
                # íŒŒì¼ ëª¨ë“œ: AudioDeviceManagerì˜ ë²„í¼ì—ì„œ ì‹œê°„ ê¸°ë°˜ìœ¼ë¡œ ì¶”ì¶œí•˜ì—¬ íŒŒì¼ ì €ì¥
                wav_path = self.audio_manager.extract_speech_segment(
                    self.speech_start_time,
                    self.last_voice_time
                )
                
                if wav_path:
                    self.node.get_logger().info(f'Speech segment saved using time-based extraction: {wav_path}')
                    self._reset_state()
                    return True, wav_path
            
            self._reset_state()
            return None

        except Exception as e:
            self.node.get_logger().error(f'Error finalizing speech (ROS): {str(e)}')
            import traceback
            traceback.print_exc()
            self._reset_state()
            return None
