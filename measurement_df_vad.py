import librosa
import librosa.display
import matplotlib.pyplot as plt
import numpy as np
import torch
from df.enhance import enhance, init_df, load_audio, save_audio
from df.utils import download_file
import sys
sys.path.insert(0, '/home/khw/workspace/Audio')
from processors.vad_engines.silero_vad import SileroVAD


def compute_vad_scores(audio, sr, vad_model):
    """
    ì˜¤ë””ì˜¤ì—ì„œ í”„ë ˆì„ë³„ VAD ìŠ¤ì½”ì–´ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
    
    Args:
        audio: ì˜¤ë””ì˜¤ ë°ì´í„° (numpy array)
        sr: ìƒ˜í”Œë ˆì´íŠ¸
        vad_model: SileroVAD ì¸ìŠ¤í„´ìŠ¤
        
    Returns:
        times: ì‹œê°„ì¶• ë°°ì—´
        scores: VAD ìŠ¤ì½”ì–´ ë°°ì—´ (0~1)
    """
    # VAD ëª¨ë¸ ìƒíƒœ ì´ˆê¸°í™”
    vad_model.frame_buffer = np.array([], dtype=np.float32)
    vad_model.score_history = []
    if hasattr(vad_model.model, 'reset_states'):
        vad_model.model.reset_states()
    
    # í”„ë ˆì„ í¬ê¸° ê³„ì‚° (SileroVADëŠ” input_samples_needed ì‚¬ìš©)
    frame_size = vad_model.input_samples_needed
    hop_size = frame_size  # non-overlapping frames
    
    scores = []
    times = []
    
    # í”„ë ˆì„ ë‹¨ìœ„ë¡œ ì²˜ë¦¬
    for i in range(0, len(audio) - frame_size, hop_size):
        frame = audio[i:i + frame_size]
        result = vad_model.process_frame(frame)
        
        if result is not None:
            is_speech, smoothed_prob, raw_prob = result
            scores.append(smoothed_prob)
            # ì‹œê°„ ê³„ì‚° (í”„ë ˆì„ ì¤‘ì‹¬ ê¸°ì¤€)
            time_sec = (i + frame_size / 2) / sr
            times.append(time_sec)
    
    return np.array(times), np.array(scores)


def visualize_and_save(original_path, enhanced_path, output_png, vad_model=None):
    """ë‘ ê°œì˜ ì˜¤ë””ì˜¤ íŒŒì¼ì„ ì½ì–´ íŒŒí˜•, ìŠ¤í™íŠ¸ë¡œê·¸ë¨, VAD ìŠ¤ì½”ì–´ë¥¼ ë¹„êµ ì €ì¥í•©ë‹ˆë‹¤."""
    
    # ë°ì´í„° ë¡œë“œ (VADìš© 48kHzì™€ ì‹œê°í™”ìš© ê¸°ë³¸ sr)
    y1, sr1 = librosa.load(original_path)
    y2, sr2 = librosa.load(enhanced_path)
    
    # VAD ìŠ¤ì½”ì–´ ê³„ì‚° (Enhancedë§Œ)
    vad_times2, vad_scores2 = None, None
    
    if vad_model is not None:
        # VADëŠ” 48kHz ì…ë ¥ í•„ìš”
        y2_48k, _ = librosa.load(enhanced_path, sr=48000)
        
        print("ğŸ¤ Enhanced ì˜¤ë””ì˜¤ VAD ìŠ¤ì½”ì–´ ê³„ì‚° ì¤‘...")
        vad_times2, vad_scores2 = compute_vad_scores(y2_48k, 48000, vad_model)
    
    # ë ˆì´ì•„ì›ƒ ì„¤ì • (VADê°€ ìˆìœ¼ë©´ 3x2, ì—†ìœ¼ë©´ 2x2)
    rows = 3 if vad_model is not None else 2
    plt.figure(figsize=(16, 5 * rows))

    # --- 1. ì›ë³¸ (Noisy) ì‹œê°í™” ---
    # íŒŒí˜•
    plt.subplot(rows, 2, 1)
    librosa.display.waveshow(y1, sr=sr1, color='gray', alpha=0.8)
    plt.title('Original (Noisy) Waveform')
    
    # ìŠ¤í™íŠ¸ë¡œê·¸ë¨
    plt.subplot(rows, 2, 3)
    D1 = librosa.stft(y1)
    S_db1 = librosa.amplitude_to_db(np.abs(D1), ref=np.max)
    librosa.display.specshow(S_db1, sr=sr1, x_axis='time', y_axis='hz', cmap='magma')
    plt.title('Original Spectrogram')

    # --- 2. ë³´ì •ë³¸ (Enhanced) ì‹œê°í™” ---
    # íŒŒí˜•
    plt.subplot(rows, 2, 2)
    librosa.display.waveshow(y2, sr=sr2, color='#800080') # ë³´ë¼ìƒ‰
    plt.title('Enhanced (DeepFilterNet2) Waveform')
    
    # ìŠ¤í™íŠ¸ë¡œê·¸ë¨
    plt.subplot(rows, 2, 4)
    D2 = librosa.stft(y2)
    S_db2 = librosa.amplitude_to_db(np.abs(D2), ref=np.max)
    librosa.display.specshow(S_db2, sr=sr2, x_axis='time', y_axis='hz', cmap='magma')
    plt.title('Enhanced Spectrogram')

    # --- 3. VAD ìŠ¤ì½”ì–´ ì‹œê°í™” (Enhancedë§Œ, 3í–‰ 2ì—´) ---
    if vad_model is not None and vad_times2 is not None:
        # 3í–‰ 1ì—´ì€ ë¹„ì›Œë‘  (subplot 5 ìŠ¤í‚µ)
        
        # Enhanced VAD (3í–‰ 2ì—´, ìœ„ì¹˜ 6)
        ax6 = plt.subplot(rows, 2, 6)
        ax6.fill_between(vad_times2, 0, vad_scores2, alpha=0.5, color='purple', label='Speech Probability')
        ax6.plot(vad_times2, vad_scores2, color='#800080', linewidth=0.8)
        ax6.axhline(y=0.5, color='red', linestyle='--', linewidth=1, label='Threshold (0.5)')
        ax6.set_xlim(0, len(y2) / sr2)  # Enhanced ì˜¤ë””ì˜¤ì™€ ë™ì¼í•œ time ìŠ¤ì¼€ì¼
        ax6.set_ylim(0, 1)
        ax6.set_xlabel('Time (s)')
        ax6.set_ylabel('Speech Probability')
        ax6.set_title('Enhanced VAD Score (Silero VAD)')
        ax6.legend(loc='upper right')
        ax6.grid(True, alpha=0.3)

    # ë ˆì´ì•„ì›ƒ ê°„ê²© ì¡°ì • (ì œëª©ê³¼ ì¶• ë¼ë²¨ ê²¹ì¹¨ ë°©ì§€)
    plt.tight_layout(pad=3.0, h_pad=3.0)
    plt.savefig(output_png, dpi=300, bbox_inches='tight')
    print(f"ğŸ“Š ì‹œê°í™” ì´ë¯¸ì§€ê°€ '{output_png}'ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    plt.show()

if __name__ == "__main__":
    # 1. DeepFilterNet ëª¨ë¸ ì´ˆê¸°í™”
    model, df_state, _ = init_df()
    
    # 2. ì˜¤ë””ì˜¤ íŒŒì¼ ì¤€ë¹„ (ì‚¬ìš©ìì˜ íŒŒì¼ ê²½ë¡œë¡œ ìˆ˜ì • ê°€ëŠ¥)
    # ì˜ˆì‹œë¥¼ ìœ„í•´ ì›ê²© íŒŒì¼ì„ ë‹¤ìš´ë¡œë“œí•˜ì§€ë§Œ, ë³¸ì¸ì˜ íŒŒì¼ì´ ìˆë‹¤ë©´ ê²½ë¡œë¥¼ ì§ì ‘ ì…ë ¥í•˜ì„¸ìš”.
    audio_path = "before_deepfilter.wav"
    
    # 3. ì˜¤ë””ì˜¤ ë¡œë“œ ë° ê°•í™”(Denoise) ì²˜ë¦¬
    audio, _ = load_audio(audio_path, sr=df_state.sr())
    enhanced = enhance(model, df_state, audio)
    
    # 4. ê²°ê³¼ íŒŒì¼ ì €ì¥
    enhanced_wav_path = "enhanced_result.wav"
    save_audio(enhanced_wav_path, enhanced, df_state.sr())
    print(f"âœ… ê°•í™”ëœ ì˜¤ë””ì˜¤ê°€ '{enhanced_wav_path}'ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    # 5. Silero VAD ëª¨ë¸ ì´ˆê¸°í™”
    print("ğŸ¤ Silero VAD ëª¨ë¸ ë¡œë”© ì¤‘...")
    vad_model = SileroVAD(
        sample_rate=48000,
        frame_duration=30,
        buffer_size=2,
        threshold=0.5
    )
    
    # 6. ì‹œê°í™” ë° PNG ì €ì¥ ì‹¤í–‰ (VAD í¬í•¨)
    visualize_and_save(audio_path, enhanced_wav_path, "comparison_result.png", vad_model)
